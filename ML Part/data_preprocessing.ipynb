{
 "cells": [
  {
   "cell_type": "code",

   "execution_count": 5,

 
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZPI0oe6BqLDF",
    "outputId": "5252059f-5c14-4331-f6fc-6ff1e429738d"
   },

   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  14%|█▍        | 10/70 [00:08<00:29,  2.04day/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not retrieve data for date 2020-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 70/70 [00:59<00:00,  1.17day/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 59.83907175064087 seconds\n"
     ]
    }
   ],

   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "base_url = \"http://agbc-fe.pdn.ac.lk/api/v1/data/?sensor=10008&date=\"\n",
    "\n",
    "start_date = pd.to_datetime(\"2020-10-22\")\n",
    "end_date = pd.to_datetime(\"2020-12-30\")\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "start_time = time.time()  # Get the current time before starting the execution\n",
    "\n",
    "for date in tqdm(date_range, desc=\"Progress\", unit=\"day\"):\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    url = base_url + date_str\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        all_data.extend(data['data'])\n",
    "    except:\n",
    "        print(f\"Error: Could not retrieve data for date {date_str}\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "end_time = time.time()  # Get the current time after finishing the execution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(\"Execution Time:\", execution_time, \"seconds\")\n",
    "\n",
    "df = pd.DataFrame(all_data, dtype=str)\n",
    "df.to_csv('dataws.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/70 [00:00<?, ?day/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not retrieve data for date 2020-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 70/70 [00:06<00:00, 10.72day/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 6.548211574554443 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "base_url = \"http://agbc-fe.pdn.ac.lk/api/v1/data/?sensor=10008&date=\"\n",
    "\n",
    "start_date = pd.to_datetime(\"2020-10-22\")\n",
    "end_date = pd.to_datetime(\"2020-12-30\")\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "def fetch_data(date):\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    url = base_url + date_str\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        return data['data']\n",
    "    except:\n",
    "        print(f\"Error: Could not retrieve data for date {date_str}\")\n",
    "        return []\n",
    "\n",
    "start_time = time.time()  # Get the current time before starting the execution\n",
    "\n",
    "\n",
    "# Create a ThreadPoolExecutor with the maximum number of workers\n",
    "executor = ThreadPoolExecutor(max_workers=None)\n",
    "\n",
    "# Use tqdm to track the progress\n",
    "with tqdm(total=len(date_range), desc=\"Progress\", unit=\"day\") as pbar:\n",
    "    # Submit the fetch_data task to the executor for each date in parallel\n",
    "    futures = [executor.submit(fetch_data, date) for date in date_range]\n",
    "\n",
    "    # Retrieve the results from the completed futures\n",
    "    for future in futures:\n",
    "        all_data.extend(future.result())\n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()  # Get the current time after finishing the execution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(\"Execution Time:\", execution_time, \"seconds\")\n",
    "\n",
    "# Create the DataFrame from the collected data\n",
    "df = pd.DataFrame(all_data, dtype=str)\n",
    "df.to_csv('dataws.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXMbj9kvrn1H",
    "outputId": "56c43792-dfe6-4cc4-9654-1b01d336140f"
   },
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBhqM74Brs51"
   },
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joYskXyPrzA8"
   },
   "outputs": [],
   "source": [
    "# Drop duplicate rows\n",
    "df=df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gEd3DbvksApo",
    "outputId": "a1a4440a-e2ab-4d00-9060-73107b549e57"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Replace '?' with NaN\n",
    "\n",
    "df.replace(' ?', np.nan, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print(df.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a new DataFrame with Average Temperature and Average Humidity Values </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3DnDwFzcrdM",
    "outputId": "93ea969a-939d-4c10-b47b-35828d743a44"
   },
   "outputs": [],
   "source": [
    "# Convert temperature columns to numeric\n",
    "df['temp1'] = pd.to_numeric(df['temp1'], errors='coerce')\n",
    "df['temp2'] = pd.to_numeric(df['temp2'], errors='coerce')\n",
    "df['temp3'] = pd.to_numeric(df['temp3'], errors='coerce')\n",
    "\n",
    "# Convert temperature columns to numeric\n",
    "df['humidity1'] = pd.to_numeric(df['humidity1'], errors='coerce')\n",
    "df['humidity2'] = pd.to_numeric(df['humidity2'], errors='coerce')\n",
    "df['humidity3'] = pd.to_numeric(df['humidity3'], errors='coerce')\n",
    "\n",
    "df['seqNo'] = pd.to_numeric(df['seqNo'], errors='coerce')\n",
    "\n",
    "# Calculate the average temperature\n",
    "df['average_internal_temp'] = df[['temp1', 'temp2', 'temp3']].mean(axis=1,skipna=True)\n",
    "\n",
    "# Calculate the average humidity\n",
    "df['average_internal_humidity'] = df[['humidity1', 'humidity2', 'humidity3']].mean(axis=1,skipna=True)\n",
    "\n",
    "# Create a new DataFrame with only the desired columns\n",
    "new_df = df[['seqNo','date','time','average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "\n",
    "\n",
    "print(new_df.head())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create a Data frame for Internal Sensor 10008 data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 'date' and 'time' columns into a single datetime column\n",
    "new_df['datetime'] = pd.to_datetime(new_df['date'] + ' ' + new_df['time'])\n",
    "# Set the 'time' column as the DataFrame index\n",
    "new_df.set_index('datetime', inplace=True)\n",
    "new_df.drop(['date', 'time','seqNo'], axis=1, inplace=True)\n",
    "# Resample the DataFrame using 'H' offset alias and select the first entry from each hour\n",
    "new_df_hourly = new_df.resample('H').first()\n",
    "\n",
    "# new_df_hourly.reset_index(inplace=True)\n",
    "# Print the resulting DataFrame\n",
    "\n",
    "new_df_hourly.to_csv('sensor10008.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create a Data frame for External Environmental data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "external_weather = pd.read_csv('weather_data.csv')\n",
    "\n",
    "# Combine the 'Date' and 'Time' columns into a single datetime column\n",
    "external_weather['datetime'] = pd.to_datetime(external_weather['Date'] + ' ' + external_weather['Time'])\n",
    "\n",
    "external_weather.drop([\"Time\",\"Date\"],axis=1,inplace=True)\n",
    "\n",
    "external_weather.set_index('datetime', inplace=True)\n",
    "\n",
    "merged_df = pd.merge(external_weather, new_df_hourly, on='datetime')\n",
    "\n",
    "# Drop rows with any null values\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "merged_df.to_csv('data_set.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Extracting features and target variable </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "columns_to_drop = ['average_internal_temp', 'average_internal_humidity', 'light', 'Clouds', 'Wind Speed','Description']\n",
    "X = merged_df.drop(columns_to_drop, axis=1)\n",
    "print(X.dtypes)\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = merged_df.corr()\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By  LinearRegression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, )\n",
    "model = LinearRegression()\n",
    "# Training the linear regression model\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)\n",
    "print(\"Model Score:\", model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By  DecisionTree Regressor</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have your input features in X and output features in y\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Create the decision tree regressor object\n",
    "clf = DecisionTreeRegressor(max_depth=4, random_state=0)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "y_pred = clf.predict(X_test)\n",
    "     \n",
    "score = r2_score(y_test, y_pred)\n",
    "print(\"Model Score:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By Lasso</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have a pandas DataFrame 'data' containing your feature columns (X) and target column (y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Create the Lasso regression model\n",
    "lasso = Lasso(alpha=0.001)  # Adjust the alpha parameter to control the degree of regularization\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "# Get the score (coefficient of determination) on the testing data\n",
    "score = lasso.score(X_test, y_test)\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Trained By Ridge regression </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have your feature matrix X and target variable y\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (optional but recommended for regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a Ridge regression model\n",
    "ridge = Ridge(alpha=1.0)  # You can adjust the regularization strength by changing the alpha parameter\n",
    "\n",
    "# Train the model\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "ed69033dcf95bf03c49614e5fe4e96f21774bed12a1dae221247a7de4d30fa71"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
