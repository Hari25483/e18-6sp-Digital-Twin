{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://agbc-fe.pdn.ac.lk/api/v1/data/?sensor=10008&date=\"\n",
    "\n",
    "start_date = pd.to_datetime(\"2020-10-22\")\n",
    "end_date = pd.to_datetime(\"2021-02-05\")\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "def fetch_data(date):\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    url = base_url + date_str\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        return data['data']\n",
    "    except:\n",
    "        print(f\"Error: Could not retrieve data for date {date_str}\")\n",
    "        return []\n",
    "\n",
    "start_time = time.time()  # Get the current time before starting the execution\n",
    "\n",
    "\n",
    "# Create a ThreadPoolExecutor with the maximum number of workers\n",
    "executor = ThreadPoolExecutor(max_workers=None)\n",
    "\n",
    "# Use tqdm to track the progress\n",
    "with tqdm(total=len(date_range), desc=\"Progress\", unit=\"day\") as pbar:\n",
    "    # Submit the fetch_data task to the executor for each date in parallel\n",
    "    futures = [executor.submit(fetch_data, date) for date in date_range]\n",
    "    \n",
    "    # Retrieve the results from the completed futures\n",
    "    for future in futures:\n",
    "        all_data.extend(future.result())\n",
    "        pbar.update(1)\n",
    "    \n",
    "\n",
    "end_time = time.time()  # Get the current time after finishing the execution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(\"Execution Time:\", execution_time, \"seconds\")\n",
    "\n",
    "# Create the DataFrame from the collected data\n",
    "df = pd.DataFrame(all_data, dtype=str)\n",
    "df.to_csv('dataws.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXMbj9kvrn1H",
    "outputId": "56c43792-dfe6-4cc4-9654-1b01d336140f"
   },
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBhqM74Brs51"
   },
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joYskXyPrzA8"
   },
   "outputs": [],
   "source": [
    "# Drop duplicate rows\n",
    "df=df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gEd3DbvksApo",
    "outputId": "a1a4440a-e2ab-4d00-9060-73107b549e57"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Replace '?' with NaN\n",
    "\n",
    "df.replace(' ?', np.nan, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print(df.tail(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a new DataFrame with Average Temperature and Average Humidity Values </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3DnDwFzcrdM",
    "outputId": "93ea969a-939d-4c10-b47b-35828d743a44"
   },
   "outputs": [],
   "source": [
    "# Convert temperature columns to numeric\n",
    "df['temp1'] = pd.to_numeric(df['temp1'], errors='coerce')\n",
    "df['temp2'] = pd.to_numeric(df['temp2'], errors='coerce')\n",
    "df['temp3'] = pd.to_numeric(df['temp3'], errors='coerce')\n",
    "\n",
    "# Convert temperature columns to numeric\n",
    "df['humidity1'] = pd.to_numeric(df['humidity1'], errors='coerce')\n",
    "df['humidity2'] = pd.to_numeric(df['humidity2'], errors='coerce')\n",
    "df['humidity3'] = pd.to_numeric(df['humidity3'], errors='coerce')\n",
    "\n",
    "df['seqNo'] = pd.to_numeric(df['seqNo'], errors='coerce')\n",
    "\n",
    "# Calculate the average temperature\n",
    "df['average_internal_temp'] = df[['temp1', 'temp2', 'temp3']].mean(axis=1,skipna=True)\n",
    "\n",
    "# Calculate the average humidity\n",
    "df['average_internal_humidity'] = df[['humidity1', 'humidity2', 'humidity3']].mean(axis=1,skipna=True)\n",
    "\n",
    "# Create a new DataFrame with only the desired columns\n",
    "new_df = df[['seqNo','date','time','average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "\n",
    "\n",
    "print(new_df.head())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create a Data frame for Internal Sensor 10008 data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 'date' and 'time' columns into a single datetime column\n",
    "new_df['datetime'] = pd.to_datetime(new_df['date'] + ' ' + new_df['time'])\n",
    "# Set the 'time' column as the DataFrame index\n",
    "new_df.set_index('datetime', inplace=True)\n",
    "new_df.drop(['date', 'time','seqNo'], axis=1, inplace=True)\n",
    "# Resample the DataFrame using 'H' offset alias and select the first entry from each hour\n",
    "new_df_hourly = new_df.resample('H').first()\n",
    "\n",
    "# new_df_hourly.reset_index(inplace=True)\n",
    "# Print the resulting DataFrame\n",
    "\n",
    "new_df_hourly.to_csv('sensor10008.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create a Data frame for External Environmental data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "external_weather = pd.read_csv('weather_data.csv')\n",
    "\n",
    "# Combine the 'Date' and 'Time' columns into a single datetime column\n",
    "external_weather['datetime'] = pd.to_datetime(external_weather['Date'] + ' ' + external_weather['Time'])\n",
    "\n",
    "external_weather.drop([\"Time\",\"Date\"],axis=1,inplace=True)\n",
    "\n",
    "external_weather.set_index('datetime', inplace=True)\n",
    "\n",
    "merged_df = pd.merge(external_weather, new_df_hourly, on='datetime')\n",
    "\n",
    "# Drop rows with any null values\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "merged_df.to_csv('data_set.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "# # Create a profile report for your DataFrame\n",
    "# profile = ProfileReport(merged_df)\n",
    "\n",
    "# # Generate the report and save it as an HTML file\n",
    "# profile.to_file('profile_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns for the box plot\n",
    "columns_to_plot = ['External Temperature', 'average_internal_temp','Feels Like']\n",
    "\n",
    "# Create the box plot using seaborn\n",
    "sns.boxplot(data=merged_df[columns_to_plot])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Box Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Extracting features and target variable </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like','Pressure','External Humidity','Dew Point','Clouds','Wind Speed']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By  LinearRegression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Training the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\",test_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By  DecisionTree Regressor</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "# Assuming you have your input features in X and output features in y\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10)\n",
    "    \n",
    "    # Create the decision tree regressor object with the suggested parameters\n",
    "    clf = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate the test accuracy\n",
    "    test_accuracy = clf.score(X_test, y_test)\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best parameters from the study\n",
    "best_params = study.best_params\n",
    "\n",
    "\n",
    "clf = DecisionTreeRegressor(max_depth=best_params['max_depth'], min_samples_leaf=best_params['min_samples_leaf'])\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best Max Depth is : \",best_params['max_depth'])\n",
    "\n",
    "print(\"Best Min Samples Leaf is : \",best_params['min_samples_leaf'])\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By Lasso</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have a pandas DataFrame 'data' containing your feature columns (X) and target column (y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Create the Lasso regression model\n",
    "lasso = Lasso(alpha=0.001)  # Adjust the alpha parameter to control the degree of regularization\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "# Get the score (coefficient of determination) on the testing data\n",
    "score = lasso.score(X_test, y_test)\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Trained By Ridge regression </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have your feature matrix X and target variable y\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (optional but recommended for regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a Ridge regression model\n",
    "ridge = Ridge(alpha=1.0)  # You can adjust the regularization strength by changing the alpha parameter\n",
    "\n",
    "# Train the model\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "ed69033dcf95bf03c49614e5fe4e96f21774bed12a1dae221247a7de4d30fa71"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
