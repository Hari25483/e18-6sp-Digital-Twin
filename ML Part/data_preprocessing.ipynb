{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/144 [00:00<?, ?day/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not retrieve data for date 2020-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 144/144 [00:12<00:00, 11.99day/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 12.012972593307495 seconds\n"
     ]
    }
   ],
   "source": [
    "base_url = \"http://agbc-fe.pdn.ac.lk/api/v1/data/?sensor=10008&date=\"\n",
    "\n",
    "start_date = pd.to_datetime(\"2020-10-22\")\n",
    "end_date = pd.to_datetime(\"2021-03-14\")\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "def fetch_data(date):\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    url = base_url + date_str\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        return data['data']\n",
    "    except:\n",
    "        print(f\"Error: Could not retrieve data for date {date_str}\")\n",
    "        return []\n",
    "\n",
    "start_time = time.time()  # Get the current time before starting the execution\n",
    "\n",
    "\n",
    "# Create a ThreadPoolExecutor with the maximum number of workers\n",
    "executor = ThreadPoolExecutor(max_workers=None)\n",
    "\n",
    "# Use tqdm to track the progress\n",
    "with tqdm(total=len(date_range), desc=\"Progress\", unit=\"day\") as pbar:\n",
    "    # Submit the fetch_data task to the executor for each date in parallel\n",
    "    futures = [executor.submit(fetch_data, date) for date in date_range]\n",
    "    \n",
    "    # Retrieve the results from the completed futures\n",
    "    for future in futures:\n",
    "        all_data.extend(future.result())\n",
    "        pbar.update(1)\n",
    "    \n",
    "\n",
    "end_time = time.time()  # Get the current time after finishing the execution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(\"Execution Time:\", execution_time, \"seconds\")\n",
    "\n",
    "# Create the DataFrame from the collected data\n",
    "df = pd.DataFrame(all_data, dtype=str)\n",
    "df.to_csv('dataws.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXMbj9kvrn1H",
    "outputId": "56c43792-dfe6-4cc4-9654-1b01d336140f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siteId       0\n",
      "seqNo        0\n",
      "date         0\n",
      "time         0\n",
      "temp1        0\n",
      "temp2        0\n",
      "temp3        0\n",
      "humidity1    0\n",
      "humidity2    0\n",
      "humidity3    0\n",
      "light        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zBhqM74Brs51"
   },
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "joYskXyPrzA8"
   },
   "outputs": [],
   "source": [
    "# Drop duplicate rows\n",
    "df=df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "gEd3DbvksApo",
    "outputId": "a1a4440a-e2ab-4d00-9060-73107b549e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       siteId  seqNo         date       time  temp1  temp2  temp3 humidity1  \\\n",
      "340154      0   2869   2021-03-14   23:55:03    NaN   26.1   24.9       NaN   \n",
      "340155      0   2870   2021-03-14   23:55:31     25   26.2   24.9      79.6   \n",
      "340156      0   2871   2021-03-14   23:56:01     25   26.1   24.8      79.5   \n",
      "340157      0   2872   2021-03-14   23:56:31     25   26.1   24.7      79.8   \n",
      "340158      0   2873   2021-03-14   23:57:01     25   26.1   24.8        80   \n",
      "340159      0   2874   2021-03-14   23:57:33    NaN   26.2   24.8       NaN   \n",
      "340160      0   2875   2021-03-14   23:58:03    NaN   26.1   24.8       NaN   \n",
      "340161      0   2876   2021-03-14   23:58:31   24.9   26.1   24.7      80.3   \n",
      "340162      0   2877   2021-03-14   23:59:01   24.9   26.1   24.8      80.4   \n",
      "340163      0   2878   2021-03-14   23:59:31   24.9   26.1   24.8      80.5   \n",
      "\n",
      "       humidity2 humidity3   light  \n",
      "340154        80        40   1.042  \n",
      "340155        80        40   1.042  \n",
      "340156        80        40   1.042  \n",
      "340157        80        41   1.042  \n",
      "340158        80        41   1.042  \n",
      "340159        81        41   1.042  \n",
      "340160        81        41   1.042  \n",
      "340161        81        42   1.042  \n",
      "340162        81        42   1.042  \n",
      "340163        81        42   1.042  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Replace '?' with NaN\n",
    "\n",
    "df.replace(' ?', np.nan, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print(df.tail(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a new DataFrame with Average Temperature and Average Humidity Values </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3DnDwFzcrdM",
    "outputId": "93ea969a-939d-4c10-b47b-35828d743a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   seqNo         date       time  average_internal_temp  \\\n",
      "0      1   2020-10-22   00:00:01              23.433333   \n",
      "1      2   2020-10-22   00:00:18              23.550000   \n",
      "2      3   2020-10-22   00:00:31              23.433333   \n",
      "3      4   2020-10-22   00:00:46              23.466667   \n",
      "4      5   2020-10-22   00:01:01              23.400000   \n",
      "\n",
      "   average_internal_humidity   light  \n",
      "0                  96.300000   1.042  \n",
      "1                  95.000000   1.042  \n",
      "2                  96.266667   1.042  \n",
      "3                  96.200000   1.042  \n",
      "4                  96.166667   1.042  \n"
     ]
    }
   ],
   "source": [
    "# Convert temperature columns to numeric\n",
    "df['temp1'] = pd.to_numeric(df['temp1'], errors='coerce')\n",
    "df['temp2'] = pd.to_numeric(df['temp2'], errors='coerce')\n",
    "df['temp3'] = pd.to_numeric(df['temp3'], errors='coerce')\n",
    "\n",
    "# Convert temperature columns to numeric\n",
    "df['humidity1'] = pd.to_numeric(df['humidity1'], errors='coerce')\n",
    "df['humidity2'] = pd.to_numeric(df['humidity2'], errors='coerce')\n",
    "df['humidity3'] = pd.to_numeric(df['humidity3'], errors='coerce')\n",
    "\n",
    "df['seqNo'] = pd.to_numeric(df['seqNo'], errors='coerce')\n",
    "\n",
    "# Calculate the average temperature\n",
    "df['average_internal_temp'] = df[['temp1', 'temp2', 'temp3']].mean(axis=1,skipna=True)\n",
    "\n",
    "# Calculate the average humidity\n",
    "df['average_internal_humidity'] = df[['humidity1', 'humidity2', 'humidity3']].mean(axis=1,skipna=True)\n",
    "\n",
    "# Create a new DataFrame with only the desired columns\n",
    "new_df = df[['seqNo','date','time','average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "\n",
    "\n",
    "print(new_df.head())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create a Data frame for Internal Sensor 10008 data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "c:\\Users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pandas\\core\\frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# Combine the 'date' and 'time' columns into a single datetime column\n",
    "new_df['datetime'] = pd.to_datetime(new_df['date'] + ' ' + new_df['time'])\n",
    "# Set the 'time' column as the DataFrame index\n",
    "new_df.set_index('datetime', inplace=True)\n",
    "new_df.drop(['date', 'time','seqNo'], axis=1, inplace=True)\n",
    "# Resample the DataFrame using 'H' offset alias and select the first entry from each hour\n",
    "new_df_hourly = new_df.resample('H').first()\n",
    "\n",
    "# new_df_hourly.reset_index(inplace=True)\n",
    "# Print the resulting DataFrame\n",
    "\n",
    "new_df_hourly.to_csv('sensor10008.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create a Data frame for External Environmental data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "external_weather = pd.read_csv('weather_data.csv')\n",
    "\n",
    "# Combine the 'Date' and 'Time' columns into a single datetime column\n",
    "external_weather['datetime'] = pd.to_datetime(external_weather['date'] + ' ' + external_weather['time'])\n",
    "\n",
    "external_weather.drop([\"time\",\"date\"],axis=1,inplace=True)\n",
    "\n",
    "external_weather.set_index('datetime', inplace=True)\n",
    "\n",
    "merged_df = pd.merge(external_weather, new_df_hourly, on='datetime')\n",
    "\n",
    "# Drop rows with any null values\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "merged_df.to_csv('data_set.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "# # Create a profile report for your DataFrame\n",
    "# profile = ProfileReport(merged_df)\n",
    "\n",
    "# # Generate the report and save it as an HTML file\n",
    "# profile.to_file('profile_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the columns for the box plot\n",
    "columns_to_plot = ['External Temperature', 'average_internal_temp','Feels Like']\n",
    "\n",
    "# Create the box plot using seaborn\n",
    "sns.boxplot(data=merged_df[columns_to_plot])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Box Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Extracting features and target variable </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like','Pressure','External Humidity','Dew Point','Clouds','Wind Speed']]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By  LinearRegression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.22630530781151162\n",
      "Test Accuracy: 0.2522579488510137\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Training the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\",test_accuracy)\n",
    "\n",
    "print(type(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9126795558788205\n",
      "Test Accuracy: 0.4268935753086518\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Training the linear regression model\n",
    "model =RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\",test_accuracy)\n",
    "\n",
    "print(type(X_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By  DecisionTree Regressor</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (2.0.15)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (0.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (1.21.5)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (1.11.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from optuna) (22.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.3.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from alembic>=1.5.0->optuna) (5.12.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-07 21:19:49,105] A new study created in memory with name: no-name-1fc0ea65-2661-46c1-ac4f-c643188405eb\n",
      "[I 2023-06-07 21:19:49,126] Trial 0 finished with value: 0.2355344738305678 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,138] Trial 1 finished with value: 0.16897677675994752 and parameters: {'max_depth': 2, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,155] Trial 2 finished with value: 0.197824114958684 and parameters: {'max_depth': 7, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,169] Trial 3 finished with value: 0.22885115631455066 and parameters: {'max_depth': 5, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,180] Trial 4 finished with value: 0.21541983546013163 and parameters: {'max_depth': 3, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,190] Trial 5 finished with value: 0.215419835460132 and parameters: {'max_depth': 3, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,206] Trial 6 finished with value: 0.12163259338415484 and parameters: {'max_depth': 9, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,220] Trial 7 finished with value: 0.21697365304092842 and parameters: {'max_depth': 6, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,238] Trial 8 finished with value: 0.19808089039143692 and parameters: {'max_depth': 10, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,253] Trial 9 finished with value: 0.2208493252803323 and parameters: {'max_depth': 6, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,281] Trial 10 finished with value: 0.23553447383056772 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,305] Trial 11 finished with value: 0.23553447383056755 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,355] Trial 12 finished with value: 0.23553447383056755 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,399] Trial 13 finished with value: 0.17076297061112486 and parameters: {'max_depth': 7, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,429] Trial 14 finished with value: 0.16897677675994704 and parameters: {'max_depth': 2, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.2355344738305678.\n",
      "[I 2023-06-07 21:19:49,465] Trial 15 finished with value: 0.23553447383056791 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,498] Trial 16 finished with value: 0.22074825073093732 and parameters: {'max_depth': 5, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,532] Trial 17 finished with value: 0.18226711485697922 and parameters: {'max_depth': 8, 'min_samples_leaf': 8}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,557] Trial 18 finished with value: 0.20556385176092284 and parameters: {'max_depth': 5, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,580] Trial 19 finished with value: 0.2154198354601321 and parameters: {'max_depth': 3, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,604] Trial 20 finished with value: 0.23553447383056772 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,636] Trial 21 finished with value: 0.23553447383056791 and parameters: {'max_depth': 4, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,661] Trial 22 finished with value: 0.20556385176092273 and parameters: {'max_depth': 5, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,683] Trial 23 finished with value: 0.21541983546013166 and parameters: {'max_depth': 3, 'min_samples_leaf': 4}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,705] Trial 24 finished with value: 0.16897677675994727 and parameters: {'max_depth': 2, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,728] Trial 25 finished with value: 0.2355344738305677 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,761] Trial 26 finished with value: 0.1811714977962925 and parameters: {'max_depth': 6, 'min_samples_leaf': 4}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,788] Trial 27 finished with value: 0.22074825073093732 and parameters: {'max_depth': 5, 'min_samples_leaf': 8}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,811] Trial 28 finished with value: 0.2154198354601317 and parameters: {'max_depth': 3, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,832] Trial 29 finished with value: 0.16897677675994757 and parameters: {'max_depth': 2, 'min_samples_leaf': 2}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,855] Trial 30 finished with value: 0.19986806637150537 and parameters: {'max_depth': 7, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,883] Trial 31 finished with value: 0.2355344738305676 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 15 with value: 0.23553447383056791.\n",
      "[I 2023-06-07 21:19:49,915] Trial 32 finished with value: 0.23553447383056794 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:49,936] Trial 33 finished with value: 0.2207482507309372 and parameters: {'max_depth': 5, 'min_samples_leaf': 8}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:49,955] Trial 34 finished with value: 0.21541983546013155 and parameters: {'max_depth': 3, 'min_samples_leaf': 5}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:49,987] Trial 35 finished with value: 0.21138619188955834 and parameters: {'max_depth': 6, 'min_samples_leaf': 8}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,013] Trial 36 finished with value: 0.23553447383056783 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,038] Trial 37 finished with value: 0.21541983546013163 and parameters: {'max_depth': 3, 'min_samples_leaf': 10}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,071] Trial 38 finished with value: 0.2041795564164013 and parameters: {'max_depth': 5, 'min_samples_leaf': 3}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,102] Trial 39 finished with value: 0.23553447383056766 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,137] Trial 40 finished with value: 0.16897677675994713 and parameters: {'max_depth': 2, 'min_samples_leaf': 4}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,182] Trial 41 finished with value: 0.23553447383056758 and parameters: {'max_depth': 4, 'min_samples_leaf': 10}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,218] Trial 42 finished with value: 0.23089325442235384 and parameters: {'max_depth': 5, 'min_samples_leaf': 9}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,244] Trial 43 finished with value: 0.21541983546013208 and parameters: {'max_depth': 3, 'min_samples_leaf': 8}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,267] Trial 44 finished with value: 0.2355344738305678 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,292] Trial 45 finished with value: 0.23553447383056744 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,319] Trial 46 finished with value: 0.19808089039143698 and parameters: {'max_depth': 10, 'min_samples_leaf': 9}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,343] Trial 47 finished with value: 0.21138619188955834 and parameters: {'max_depth': 6, 'min_samples_leaf': 8}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,365] Trial 48 finished with value: 0.17076297061112497 and parameters: {'max_depth': 7, 'min_samples_leaf': 6}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,392] Trial 49 finished with value: 0.21541983546013202 and parameters: {'max_depth': 3, 'min_samples_leaf': 7}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,452] Trial 50 finished with value: 0.22885115631455055 and parameters: {'max_depth': 5, 'min_samples_leaf': 10}. Best is trial 32 with value: 0.23553447383056794.\n",
      "[I 2023-06-07 21:19:50,491] Trial 51 finished with value: 0.23553447383056805 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,518] Trial 52 finished with value: 0.2355344738305676 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,551] Trial 53 finished with value: 0.21541983546013202 and parameters: {'max_depth': 3, 'min_samples_leaf': 5}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,584] Trial 54 finished with value: 0.23553447383056783 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,621] Trial 55 finished with value: 0.2355344738305675 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,663] Trial 56 finished with value: 0.21202567045429724 and parameters: {'max_depth': 5, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,699] Trial 57 finished with value: 0.21541983546013221 and parameters: {'max_depth': 3, 'min_samples_leaf': 5}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,726] Trial 58 finished with value: 0.2355344738305679 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,750] Trial 59 finished with value: 0.2207482507309374 and parameters: {'max_depth': 5, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,778] Trial 60 finished with value: 0.1547803987325391 and parameters: {'max_depth': 8, 'min_samples_leaf': 5}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,804] Trial 61 finished with value: 0.2355344738305677 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,832] Trial 62 finished with value: 0.23553447383056783 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,858] Trial 63 finished with value: 0.23553447383056755 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,881] Trial 64 finished with value: 0.23553447383056794 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,907] Trial 65 finished with value: 0.2154198354601318 and parameters: {'max_depth': 3, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,937] Trial 66 finished with value: 0.22074825073093732 and parameters: {'max_depth': 5, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,965] Trial 67 finished with value: 0.23553447383056758 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:50,992] Trial 68 finished with value: 0.2154198354601318 and parameters: {'max_depth': 3, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,023] Trial 69 finished with value: 0.20417955641640126 and parameters: {'max_depth': 5, 'min_samples_leaf': 4}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,063] Trial 70 finished with value: 0.211253550013766 and parameters: {'max_depth': 6, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,096] Trial 71 finished with value: 0.23553447383056783 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,146] Trial 72 finished with value: 0.2355344738305676 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,181] Trial 73 finished with value: 0.2355344738305675 and parameters: {'max_depth': 4, 'min_samples_leaf': 5}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,206] Trial 74 finished with value: 0.21541983546013208 and parameters: {'max_depth': 3, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,233] Trial 75 finished with value: 0.23553447383056747 and parameters: {'max_depth': 4, 'min_samples_leaf': 5}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,258] Trial 76 finished with value: 0.2207482507309373 and parameters: {'max_depth': 5, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,282] Trial 77 finished with value: 0.215419835460132 and parameters: {'max_depth': 3, 'min_samples_leaf': 3}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,304] Trial 78 finished with value: 0.16897677675994718 and parameters: {'max_depth': 2, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,330] Trial 79 finished with value: 0.23553447383056778 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,363] Trial 80 finished with value: 0.2120256704542972 and parameters: {'max_depth': 5, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,390] Trial 81 finished with value: 0.23553447383056755 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,415] Trial 82 finished with value: 0.2355344738305678 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,445] Trial 83 finished with value: 0.23553447383056755 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,480] Trial 84 finished with value: 0.23553447383056725 and parameters: {'max_depth': 4, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,521] Trial 85 finished with value: 0.22074825073093726 and parameters: {'max_depth': 5, 'min_samples_leaf': 7}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,561] Trial 86 finished with value: 0.23553447383056772 and parameters: {'max_depth': 4, 'min_samples_leaf': 6}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,602] Trial 87 finished with value: 0.21541983546013174 and parameters: {'max_depth': 3, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,654] Trial 88 finished with value: 0.23553447383056805 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,700] Trial 89 finished with value: 0.22074825073093737 and parameters: {'max_depth': 5, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,745] Trial 90 finished with value: 0.20675165419287878 and parameters: {'max_depth': 9, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,770] Trial 91 finished with value: 0.2355344738305679 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,797] Trial 92 finished with value: 0.2355344738305676 and parameters: {'max_depth': 4, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,821] Trial 93 finished with value: 0.2355344738305679 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,846] Trial 94 finished with value: 0.2355344738305677 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,871] Trial 95 finished with value: 0.21541983546013174 and parameters: {'max_depth': 3, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,897] Trial 96 finished with value: 0.22885115631455033 and parameters: {'max_depth': 5, 'min_samples_leaf': 10}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,925] Trial 97 finished with value: 0.215419835460132 and parameters: {'max_depth': 3, 'min_samples_leaf': 8}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,964] Trial 98 finished with value: 0.2355344738305677 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n",
      "[I 2023-06-07 21:19:51,999] Trial 99 finished with value: 0.23553447383056766 and parameters: {'max_depth': 4, 'min_samples_leaf': 9}. Best is trial 51 with value: 0.23553447383056805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Max Depth is :  4\n",
      "Best Min Samples Leaf is :  7\n",
      "Training Accuracy: 0.365247381232352\n",
      "Test Accuracy: 0.23553447383056728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "# Assuming you have your input features in X and output features in y\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10)\n",
    "    \n",
    "    # Create the decision tree regressor object with the suggested parameters\n",
    "    clf = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate the test accuracy\n",
    "    test_accuracy = clf.score(X_test, y_test)\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best parameters from the study\n",
    "best_params = study.best_params\n",
    "\n",
    "\n",
    "clf = DecisionTreeRegressor(max_depth=best_params['max_depth'], min_samples_leaf=best_params['min_samples_leaf'])\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best Max Depth is : \",best_params['max_depth'])\n",
    "\n",
    "print(\"Best Min Samples Leaf is : \",best_params['min_samples_leaf'])\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Trained By Lasso</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9205226.375951892\n",
      "Score: 0.21910478812166045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have a pandas DataFrame 'data' containing your feature columns (X) and target column (y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Create the Lasso regression model\n",
    "lasso = Lasso(alpha=0.001)  # Adjust the alpha parameter to control the degree of regularization\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "# Get the score (coefficient of determination) on the testing data\n",
    "score = lasso.score(X_test, y_test)\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Trained By Ridge regression </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9199522.69808088\n",
      "R2 Score: 0.21962995795551687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have your feature matrix X and target variable y\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (optional but recommended for regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a Ridge regression model\n",
    "ridge = Ridge(alpha=1.0)  # You can adjust the regularization strength by changing the alpha parameter\n",
    "\n",
    "# Train the model\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MLPRegressor</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -0.0011481089061574634\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have your feature matrix X and target variable y\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like','Pressure','External Humidity','Dew Point','Clouds','Wind Speed']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#\n",
    "# Create the model\n",
    "model = MLPRegressor(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target values for the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.score(X_test, y_test)\n",
    "print('Score:', score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RANDOM FOREST REGRESSOR</h1> 39% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4105804367043759\n",
      "Training Accuracy: 0.7917583974727208\n",
      "Test Accuracy: 0.4105804367043759\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have your feature matrix X and target variable y\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like','Pressure','External Humidity','Dew Point','Clouds','Wind Speed']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#\n",
    "# Create the model\n",
    "model =RandomForestRegressor(n_estimators=150, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target values for the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.score(X_test, y_test)\n",
    "print('Score:', score)\n",
    "\n",
    "\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp37-cp37m-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp37-cp37m-win_amd64.whl (266.3 MB)\n",
      "     -------------------------------------- 266.3/266.3 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.2-cp37-cp37m-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     -------------------------------------- 439.2/439.2 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 4.3 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 4.1 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ---------------------------------------- 126.5/126.5 kB ? eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     ---------------------------------------- 24.4/24.4 MB 4.2 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-win_amd64.whl (896 kB)\n",
      "     -------------------------------------- 896.6/896.6 kB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.6.3)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.9/93.9 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.19.1-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.3/181.3 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.3/181.3 kB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.14)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\vilak\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.11.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.9/83.9 kB ? eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.19.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.54.2 h5py-3.8.0 keras-2.11.0 libclang-16.0.0 markdown-3.4.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4628\\979264105.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# **Simple neural network**\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0.post5-py3-none-any.whl size=2970 sha256=7b142af98f9f9fa06f67c4ea6a8f3a720e69f40256ec2216bc0d5c3d1d7dfec0\n",
      "  Stored in directory: c:\\users\\vilak\\appdata\\local\\pip\\cache\\wheels\\b2\\af\\1b\\ac28f3fb36a8428e3089acdd913e9ee1808e781e3ff6ce2929\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post5\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post5-py3-none-any.whl\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post5\n"
     ]
    }
   ],
   "source": [
    "# **Simple neural network**\n",
    "\n",
    "import tensorflow as tf\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
    "\n",
    "# Convert the X_train and y_train variables to type float32\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "X_train= tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train= tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "# **Deep neural network**\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Convert the X_train and y_train variables to type float32\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/hari/anaconda3/lib/python3.10/site-packages (2.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 12632283.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 2/10, Loss: 11679390.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 3/10, Loss: 11610776.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 4/10, Loss: 11662118.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 5/10, Loss: 11680528.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 6/10, Loss: 11602964.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 7/10, Loss: 11607462.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 8/10, Loss: 11577952.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 9/10, Loss: 11553023.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 10/10, Loss: 11655988.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "18/18 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Load the input and output data from the pandas DataFrame\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like', 'Pressure', 'External Humidity', 'Dew Point', 'Clouds', 'Wind Speed']]\n",
    "\n",
    "# Convert the pandas DataFrame to numpy arrays with float32 data type\n",
    "X = X.astype('float32').to_numpy()\n",
    "y = y.astype('float32').to_numpy()\n",
    "\n",
    "# Define the model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(6,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(3))  # Output layer with 3 units for the 3 output columns\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_y, test_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and compute the loss\n",
    "    history = model.fit(train_X, train_y, batch_size=32, epochs=1, verbose=0)\n",
    "\n",
    "    # Compute training and testing accuracy\n",
    "    train_loss, train_accuracy = model.evaluate(train_X, train_y, verbose=0)\n",
    "    test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=0)\n",
    "\n",
    "    # Print the loss and accuracy after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {history.history['loss'][0]}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Evaluation\n",
    "predictions = model.predict(test_X)\n",
    "# Perform further evaluation or analysis on the predictions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Architecture similar to VGG net</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 12535148.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 2/300, Loss: 12312753.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 3/300, Loss: 11845016.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 4/300, Loss: 12516900.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 5/300, Loss: 11783011.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 6/300, Loss: 11889423.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 7/300, Loss: 11637256.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 8/300, Loss: 12030151.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 9/300, Loss: 11585359.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 10/300, Loss: 11251740.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 11/300, Loss: 11285540.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 12/300, Loss: 10525945.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 13/300, Loss: 10280865.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 14/300, Loss: 10880593.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 15/300, Loss: 10046830.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 16/300, Loss: 10162475.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 17/300, Loss: 9923013.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 18/300, Loss: 10044508.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 19/300, Loss: 10046942.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 20/300, Loss: 10158528.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 21/300, Loss: 9707653.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 22/300, Loss: 9722559.0, Train Accuracy: 0.4670184552669525, Test Accuracy: 0.5061511397361755\n",
      "Epoch 23/300, Loss: 9749335.0, Train Accuracy: 0.45778363943099976, Test Accuracy: 0.49384886026382446\n",
      "Epoch 24/300, Loss: 9688712.0, Train Accuracy: 0.4890061616897583, Test Accuracy: 0.5430579781532288\n",
      "Epoch 25/300, Loss: 10097868.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 26/300, Loss: 10427467.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 27/300, Loss: 9624706.0, Train Accuracy: 0.5637642741203308, Test Accuracy: 0.5834797620773315\n",
      "Epoch 28/300, Loss: 9724446.0, Train Accuracy: 0.4617414176464081, Test Accuracy: 0.5079085826873779\n",
      "Epoch 29/300, Loss: 9870234.0, Train Accuracy: 0.4643799364566803, Test Accuracy: 0.5114235281944275\n",
      "Epoch 30/300, Loss: 9683879.0, Train Accuracy: 0.4793315827846527, Test Accuracy: 0.5325132012367249\n",
      "Epoch 31/300, Loss: 9778782.0, Train Accuracy: 0.4841688573360443, Test Accuracy: 0.5307556986808777\n",
      "Epoch 32/300, Loss: 9614236.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 33/300, Loss: 9542402.0, Train Accuracy: 0.47845205664634705, Test Accuracy: 0.5395430326461792\n",
      "Epoch 34/300, Loss: 9954165.0, Train Accuracy: 0.4507475793361664, Test Accuracy: 0.4920913875102997\n",
      "Epoch 35/300, Loss: 9672959.0, Train Accuracy: 0.536499559879303, Test Accuracy: 0.5799648761749268\n",
      "Epoch 36/300, Loss: 9574750.0, Train Accuracy: 0.46657872200012207, Test Accuracy: 0.5166959762573242\n",
      "Epoch 37/300, Loss: 9517026.0, Train Accuracy: 0.5932278037071228, Test Accuracy: 0.6379613280296326\n",
      "Epoch 38/300, Loss: 9637296.0, Train Accuracy: 0.5043975114822388, Test Accuracy: 0.5641476511955261\n",
      "Epoch 39/300, Loss: 10135523.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 40/300, Loss: 9549132.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 41/300, Loss: 9544206.0, Train Accuracy: 0.5400176048278809, Test Accuracy: 0.5834797620773315\n",
      "Epoch 42/300, Loss: 9870704.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 43/300, Loss: 9524074.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 44/300, Loss: 9614366.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 45/300, Loss: 9768785.0, Train Accuracy: 0.5123131275177002, Test Accuracy: 0.5588752031326294\n",
      "Epoch 46/300, Loss: 9712459.0, Train Accuracy: 0.5290237665176392, Test Accuracy: 0.557117760181427\n",
      "Epoch 47/300, Loss: 9716996.0, Train Accuracy: 0.4569041430950165, Test Accuracy: 0.502636194229126\n",
      "Epoch 48/300, Loss: 9879411.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 49/300, Loss: 9493393.0, Train Accuracy: 0.5206684470176697, Test Accuracy: 0.5606327056884766\n",
      "Epoch 50/300, Loss: 9556693.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 51/300, Loss: 9889179.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 52/300, Loss: 9595498.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 53/300, Loss: 9434883.0, Train Accuracy: 0.5910290479660034, Test Accuracy: 0.6256590485572815\n",
      "Epoch 54/300, Loss: 9984335.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 55/300, Loss: 9423799.0, Train Accuracy: 0.4727352559566498, Test Accuracy: 0.5307556986808777\n",
      "Epoch 56/300, Loss: 9547128.0, Train Accuracy: 0.47757256031036377, Test Accuracy: 0.5395430326461792\n",
      "Epoch 57/300, Loss: 9564153.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 58/300, Loss: 10089968.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 59/300, Loss: 9490245.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 60/300, Loss: 9461587.0, Train Accuracy: 0.5127528309822083, Test Accuracy: 0.5606327056884766\n",
      "Epoch 61/300, Loss: 9339111.0, Train Accuracy: 0.5782761573791504, Test Accuracy: 0.6221441030502319\n",
      "Epoch 62/300, Loss: 9393164.0, Train Accuracy: 0.4960422217845917, Test Accuracy: 0.5606327056884766\n",
      "Epoch 63/300, Loss: 9379917.0, Train Accuracy: 0.5017589926719666, Test Accuracy: 0.5606327056884766\n",
      "Epoch 64/300, Loss: 9426233.0, Train Accuracy: 0.5439753532409668, Test Accuracy: 0.6063268780708313\n",
      "Epoch 65/300, Loss: 9658999.0, Train Accuracy: 0.4912049174308777, Test Accuracy: 0.5430579781532288\n",
      "Epoch 66/300, Loss: 9344459.0, Train Accuracy: 0.4991205036640167, Test Accuracy: 0.5553602576255798\n",
      "Epoch 67/300, Loss: 9401675.0, Train Accuracy: 0.5026385188102722, Test Accuracy: 0.5694200396537781\n",
      "Epoch 68/300, Loss: 9461497.0, Train Accuracy: 0.5501319169998169, Test Accuracy: 0.6080843806266785\n",
      "Epoch 69/300, Loss: 9226600.0, Train Accuracy: 0.572999119758606, Test Accuracy: 0.6326889395713806\n",
      "Epoch 70/300, Loss: 9461862.0, Train Accuracy: 0.4753738045692444, Test Accuracy: 0.5307556986808777\n",
      "Epoch 71/300, Loss: 9670305.0, Train Accuracy: 0.4617414176464081, Test Accuracy: 0.5184534192085266\n",
      "Epoch 72/300, Loss: 9339487.0, Train Accuracy: 0.4898856580257416, Test Accuracy: 0.5641476511955261\n",
      "Epoch 73/300, Loss: 9618970.0, Train Accuracy: 0.49076518416404724, Test Accuracy: 0.5536028146743774\n",
      "Epoch 74/300, Loss: 9365022.0, Train Accuracy: 0.5105540752410889, Test Accuracy: 0.5729349851608276\n",
      "Epoch 75/300, Loss: 9617404.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 76/300, Loss: 9433983.0, Train Accuracy: 0.4925241768360138, Test Accuracy: 0.5659050941467285\n",
      "Epoch 77/300, Loss: 9447783.0, Train Accuracy: 0.509674608707428, Test Accuracy: 0.5782073736190796\n",
      "Epoch 78/300, Loss: 9504520.0, Train Accuracy: 0.5039578080177307, Test Accuracy: 0.562390148639679\n",
      "Epoch 79/300, Loss: 9421353.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 80/300, Loss: 9484925.0, Train Accuracy: 0.48372912406921387, Test Accuracy: 0.5536028146743774\n",
      "Epoch 81/300, Loss: 9381250.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 82/300, Loss: 9217379.0, Train Accuracy: 0.6754617691040039, Test Accuracy: 0.7100175619125366\n",
      "Epoch 83/300, Loss: 9448787.0, Train Accuracy: 0.4705365002155304, Test Accuracy: 0.5307556986808777\n",
      "Epoch 84/300, Loss: 9436892.0, Train Accuracy: 0.4621811807155609, Test Accuracy: 0.5166959762573242\n",
      "Epoch 85/300, Loss: 9343024.0, Train Accuracy: 0.4731750190258026, Test Accuracy: 0.5377855896949768\n",
      "Epoch 86/300, Loss: 9528477.0, Train Accuracy: 0.48812663555145264, Test Accuracy: 0.557117760181427\n",
      "Epoch 87/300, Loss: 9258377.0, Train Accuracy: 0.5021987557411194, Test Accuracy: 0.5694200396537781\n",
      "Epoch 88/300, Loss: 9188040.0, Train Accuracy: 0.4806508421897888, Test Accuracy: 0.5500878691673279\n",
      "Epoch 89/300, Loss: 9397950.0, Train Accuracy: 0.6064203977584839, Test Accuracy: 0.6449912190437317\n",
      "Epoch 90/300, Loss: 9190219.0, Train Accuracy: 0.4916446805000305, Test Accuracy: 0.562390148639679\n",
      "Epoch 91/300, Loss: 9214513.0, Train Accuracy: 0.5162708759307861, Test Accuracy: 0.5782073736190796\n",
      "Epoch 92/300, Loss: 9280680.0, Train Accuracy: 0.586191713809967, Test Accuracy: 0.6274164915084839\n",
      "Epoch 93/300, Loss: 9189225.0, Train Accuracy: 0.5021987557411194, Test Accuracy: 0.5676625370979309\n",
      "Epoch 94/300, Loss: 9165449.0, Train Accuracy: 0.5277044773101807, Test Accuracy: 0.5922671556472778\n",
      "Epoch 95/300, Loss: 9530044.0, Train Accuracy: 0.5606859922409058, Test Accuracy: 0.6133567690849304\n",
      "Epoch 96/300, Loss: 9182077.0, Train Accuracy: 0.4964819848537445, Test Accuracy: 0.5606327056884766\n",
      "Epoch 97/300, Loss: 9327983.0, Train Accuracy: 0.49076518416404724, Test Accuracy: 0.562390148639679\n",
      "Epoch 98/300, Loss: 9194899.0, Train Accuracy: 0.5954265594482422, Test Accuracy: 0.6309314370155334\n",
      "Epoch 99/300, Loss: 9113687.0, Train Accuracy: 0.48372912406921387, Test Accuracy: 0.5588752031326294\n",
      "Epoch 100/300, Loss: 9402873.0, Train Accuracy: 0.4494283199310303, Test Accuracy: 0.4920913875102997\n",
      "Epoch 101/300, Loss: 9099965.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 102/300, Loss: 9235348.0, Train Accuracy: 0.48680737614631653, Test Accuracy: 0.562390148639679\n",
      "Epoch 103/300, Loss: 9291147.0, Train Accuracy: 0.47977131605148315, Test Accuracy: 0.5553602576255798\n",
      "Epoch 104/300, Loss: 9303276.0, Train Accuracy: 0.4815303385257721, Test Accuracy: 0.5553602576255798\n",
      "Epoch 105/300, Loss: 9373498.0, Train Accuracy: 0.503078281879425, Test Accuracy: 0.5694200396537781\n",
      "Epoch 106/300, Loss: 9275639.0, Train Accuracy: 0.480211079120636, Test Accuracy: 0.5448154807090759\n",
      "Epoch 107/300, Loss: 9137318.0, Train Accuracy: 0.4766930639743805, Test Accuracy: 0.5448154807090759\n",
      "Epoch 108/300, Loss: 9177496.0, Train Accuracy: 0.4929639399051666, Test Accuracy: 0.562390148639679\n",
      "Epoch 109/300, Loss: 9123057.0, Train Accuracy: 0.6710641980171204, Test Accuracy: 0.6731107234954834\n",
      "Epoch 110/300, Loss: 9224426.0, Train Accuracy: 0.5219876766204834, Test Accuracy: 0.5817223191261292\n",
      "Epoch 111/300, Loss: 9025602.0, Train Accuracy: 0.5417765974998474, Test Accuracy: 0.5817223191261292\n",
      "Epoch 112/300, Loss: 8894475.0, Train Accuracy: 0.5496921539306641, Test Accuracy: 0.6063268780708313\n",
      "Epoch 113/300, Loss: 9135443.0, Train Accuracy: 0.5162708759307861, Test Accuracy: 0.5817223191261292\n",
      "Epoch 114/300, Loss: 9060339.0, Train Accuracy: 0.5184696316719055, Test Accuracy: 0.5711774826049805\n",
      "Epoch 115/300, Loss: 8981660.0, Train Accuracy: 0.6094986796379089, Test Accuracy: 0.6239016056060791\n",
      "Epoch 116/300, Loss: 9308351.0, Train Accuracy: 0.4916446805000305, Test Accuracy: 0.5377855896949768\n",
      "Epoch 117/300, Loss: 9105159.0, Train Accuracy: 0.6429199576377869, Test Accuracy: 0.6555360555648804\n",
      "Epoch 118/300, Loss: 9134089.0, Train Accuracy: 0.4815303385257721, Test Accuracy: 0.5413005352020264\n",
      "Epoch 119/300, Loss: 9001982.0, Train Accuracy: 0.5026385188102722, Test Accuracy: 0.5588752031326294\n",
      "Epoch 120/300, Loss: 8932172.0, Train Accuracy: 0.4788918197154999, Test Accuracy: 0.5307556986808777\n",
      "Epoch 121/300, Loss: 9079041.0, Train Accuracy: 0.4986807405948639, Test Accuracy: 0.5483304262161255\n",
      "Epoch 122/300, Loss: 9039844.0, Train Accuracy: 0.5545294880867004, Test Accuracy: 0.5887522101402283\n",
      "Epoch 123/300, Loss: 9185917.0, Train Accuracy: 0.5404573678970337, Test Accuracy: 0.5852372646331787\n",
      "Epoch 124/300, Loss: 8745109.0, Train Accuracy: 0.4520668387413025, Test Accuracy: 0.48506152629852295\n",
      "Epoch 125/300, Loss: 9183941.0, Train Accuracy: 0.49560245871543884, Test Accuracy: 0.5483304262161255\n",
      "Epoch 126/300, Loss: 9114738.0, Train Accuracy: 0.4498680830001831, Test Accuracy: 0.48330405354499817\n",
      "Epoch 127/300, Loss: 8828997.0, Train Accuracy: 0.5145118832588196, Test Accuracy: 0.5588752031326294\n",
      "Epoch 128/300, Loss: 8980470.0, Train Accuracy: 0.5127528309822083, Test Accuracy: 0.5694200396537781\n",
      "Epoch 129/300, Loss: 9066785.0, Train Accuracy: 0.4890061616897583, Test Accuracy: 0.5413005352020264\n",
      "Epoch 130/300, Loss: 9278025.0, Train Accuracy: 0.5954265594482422, Test Accuracy: 0.6098418235778809\n",
      "Epoch 131/300, Loss: 9018468.0, Train Accuracy: 0.4903254210948944, Test Accuracy: 0.5500878691673279\n",
      "Epoch 132/300, Loss: 8798036.0, Train Accuracy: 0.5087950825691223, Test Accuracy: 0.5711774826049805\n",
      "Epoch 133/300, Loss: 8840824.0, Train Accuracy: 0.5127528309822083, Test Accuracy: 0.5729349851608276\n",
      "Epoch 134/300, Loss: 8868344.0, Train Accuracy: 0.5686016082763672, Test Accuracy: 0.6028119325637817\n",
      "Epoch 135/300, Loss: 9007223.0, Train Accuracy: 0.4582234025001526, Test Accuracy: 0.5043936967849731\n",
      "Epoch 136/300, Loss: 8929562.0, Train Accuracy: 0.6288478374481201, Test Accuracy: 0.634446382522583\n",
      "Epoch 137/300, Loss: 9052114.0, Train Accuracy: 0.48372912406921387, Test Accuracy: 0.5430579781532288\n",
      "Epoch 138/300, Loss: 8959406.0, Train Accuracy: 0.5505716800689697, Test Accuracy: 0.6362038850784302\n",
      "Epoch 139/300, Loss: 8854915.0, Train Accuracy: 0.4709762632846832, Test Accuracy: 0.5254833102226257\n",
      "Epoch 140/300, Loss: 8844778.0, Train Accuracy: 0.565963089466095, Test Accuracy: 0.5940245985984802\n",
      "Epoch 141/300, Loss: 8743437.0, Train Accuracy: 0.4938434362411499, Test Accuracy: 0.5659050941467285\n",
      "Epoch 142/300, Loss: 8932864.0, Train Accuracy: 0.519788920879364, Test Accuracy: 0.5834797620773315\n",
      "Epoch 143/300, Loss: 8688943.0, Train Accuracy: 0.5875110030174255, Test Accuracy: 0.61687171459198\n",
      "Epoch 144/300, Loss: 8656360.0, Train Accuracy: 0.6715039610862732, Test Accuracy: 0.694200336933136\n",
      "Epoch 145/300, Loss: 9009397.0, Train Accuracy: 0.5131925940513611, Test Accuracy: 0.5922671556472778\n",
      "Epoch 146/300, Loss: 8742339.0, Train Accuracy: 0.495162695646286, Test Accuracy: 0.5553602576255798\n",
      "Epoch 147/300, Loss: 9171568.0, Train Accuracy: 0.5171504020690918, Test Accuracy: 0.5694200396537781\n",
      "Epoch 148/300, Loss: 8850271.0, Train Accuracy: 0.5769569277763367, Test Accuracy: 0.6063268780708313\n",
      "Epoch 149/300, Loss: 8737717.0, Train Accuracy: 0.5400176048278809, Test Accuracy: 0.6186291575431824\n",
      "Epoch 150/300, Loss: 8578081.0, Train Accuracy: 0.48592787981033325, Test Accuracy: 0.551845371723175\n",
      "Epoch 151/300, Loss: 8691144.0, Train Accuracy: 0.6165347695350647, Test Accuracy: 0.6362038850784302\n",
      "Epoch 152/300, Loss: 9157810.0, Train Accuracy: 0.612576961517334, Test Accuracy: 0.6537785530090332\n",
      "Epoch 153/300, Loss: 8777679.0, Train Accuracy: 0.5492523908615112, Test Accuracy: 0.5782073736190796\n",
      "Epoch 154/300, Loss: 8923757.0, Train Accuracy: 0.5989446043968201, Test Accuracy: 0.6045694351196289\n",
      "Epoch 155/300, Loss: 8610357.0, Train Accuracy: 0.49956023693084717, Test Accuracy: 0.5553602576255798\n",
      "Epoch 156/300, Loss: 8982508.0, Train Accuracy: 0.46877747774124146, Test Accuracy: 0.514938473701477\n",
      "Epoch 157/300, Loss: 8535969.0, Train Accuracy: 0.47757256031036377, Test Accuracy: 0.5325132012367249\n",
      "Epoch 158/300, Loss: 8842068.0, Train Accuracy: 0.6671064496040344, Test Accuracy: 0.676625669002533\n",
      "Epoch 159/300, Loss: 9025533.0, Train Accuracy: 0.45426562428474426, Test Accuracy: 0.5008787512779236\n",
      "Epoch 160/300, Loss: 8753962.0, Train Accuracy: 0.5092348456382751, Test Accuracy: 0.5694200396537781\n",
      "Epoch 161/300, Loss: 8848038.0, Train Accuracy: 0.5708003640174866, Test Accuracy: 0.5887522101402283\n",
      "Epoch 162/300, Loss: 8628495.0, Train Accuracy: 0.47141599655151367, Test Accuracy: 0.5395430326461792\n",
      "Epoch 163/300, Loss: 8926843.0, Train Accuracy: 0.4520668387413025, Test Accuracy: 0.5008787512779236\n",
      "Epoch 164/300, Loss: 8807067.0, Train Accuracy: 0.5013192892074585, Test Accuracy: 0.5641476511955261\n",
      "Epoch 165/300, Loss: 8866277.0, Train Accuracy: 0.4929639399051666, Test Accuracy: 0.5659050941467285\n",
      "Epoch 166/300, Loss: 8741394.0, Train Accuracy: 0.5822339653968811, Test Accuracy: 0.6098418235778809\n",
      "Epoch 167/300, Loss: 8824699.0, Train Accuracy: 0.5426561236381531, Test Accuracy: 0.5817223191261292\n",
      "Epoch 168/300, Loss: 8624823.0, Train Accuracy: 0.5888302326202393, Test Accuracy: 0.6221441030502319\n",
      "Epoch 169/300, Loss: 8584094.0, Train Accuracy: 0.5123131275177002, Test Accuracy: 0.5869947075843811\n",
      "Epoch 170/300, Loss: 8609977.0, Train Accuracy: 0.4617414176464081, Test Accuracy: 0.5096660852432251\n",
      "Epoch 171/300, Loss: 8745954.0, Train Accuracy: 0.5250659584999084, Test Accuracy: 0.5887522101402283\n",
      "Epoch 172/300, Loss: 8701012.0, Train Accuracy: 0.602462649345398, Test Accuracy: 0.6256590485572815\n",
      "Epoch 173/300, Loss: 8690475.0, Train Accuracy: 0.5448548793792725, Test Accuracy: 0.611599326133728\n",
      "Epoch 174/300, Loss: 8702099.0, Train Accuracy: 0.5246261954307556, Test Accuracy: 0.5975395441055298\n",
      "Epoch 175/300, Loss: 8669062.0, Train Accuracy: 0.45426562428474426, Test Accuracy: 0.4991212785243988\n",
      "Epoch 176/300, Loss: 8641282.0, Train Accuracy: 0.4841688573360443, Test Accuracy: 0.5588752031326294\n",
      "Epoch 177/300, Loss: 8627869.0, Train Accuracy: 0.5439753532409668, Test Accuracy: 0.61687171459198\n",
      "Epoch 178/300, Loss: 8616361.0, Train Accuracy: 0.5158311128616333, Test Accuracy: 0.5852372646331787\n",
      "Epoch 179/300, Loss: 8397285.0, Train Accuracy: 0.4964819848537445, Test Accuracy: 0.5694200396537781\n",
      "Epoch 180/300, Loss: 8415572.0, Train Accuracy: 0.4806508421897888, Test Accuracy: 0.5500878691673279\n",
      "Epoch 181/300, Loss: 8456674.0, Train Accuracy: 0.6187335252761841, Test Accuracy: 0.6432337164878845\n",
      "Epoch 182/300, Loss: 8671440.0, Train Accuracy: 0.4696570038795471, Test Accuracy: 0.5289982557296753\n",
      "Epoch 183/300, Loss: 8644644.0, Train Accuracy: 0.4916446805000305, Test Accuracy: 0.5588752031326294\n",
      "Epoch 184/300, Loss: 8472500.0, Train Accuracy: 0.48460862040519714, Test Accuracy: 0.5588752031326294\n",
      "Epoch 185/300, Loss: 8677190.0, Train Accuracy: 0.4986807405948639, Test Accuracy: 0.5729349851608276\n",
      "Epoch 186/300, Loss: 8952242.0, Train Accuracy: 0.48460862040519714, Test Accuracy: 0.5360281467437744\n",
      "Epoch 187/300, Loss: 8867005.0, Train Accuracy: 0.5378188490867615, Test Accuracy: 0.6080843806266785\n",
      "Epoch 188/300, Loss: 8589740.0, Train Accuracy: 0.5127528309822083, Test Accuracy: 0.5940245985984802\n",
      "Epoch 189/300, Loss: 8578278.0, Train Accuracy: 0.4841688573360443, Test Accuracy: 0.5641476511955261\n",
      "Epoch 190/300, Loss: 8974988.0, Train Accuracy: 0.5839929580688477, Test Accuracy: 0.5940245985984802\n",
      "Epoch 191/300, Loss: 8547480.0, Train Accuracy: 0.5430958867073059, Test Accuracy: 0.6151142120361328\n",
      "Epoch 192/300, Loss: 8706906.0, Train Accuracy: 0.5527704358100891, Test Accuracy: 0.5975395441055298\n",
      "Epoch 193/300, Loss: 8693373.0, Train Accuracy: 0.6213720440864563, Test Accuracy: 0.6239016056060791\n",
      "Epoch 194/300, Loss: 8517484.0, Train Accuracy: 0.5307827591896057, Test Accuracy: 0.5764499306678772\n",
      "Epoch 195/300, Loss: 8623922.0, Train Accuracy: 0.5360597968101501, Test Accuracy: 0.6239016056060791\n",
      "Epoch 196/300, Loss: 8461199.0, Train Accuracy: 0.5734388828277588, Test Accuracy: 0.6098418235778809\n",
      "Epoch 197/300, Loss: 8917394.0, Train Accuracy: 0.5035180449485779, Test Accuracy: 0.5659050941467285\n",
      "Epoch 198/300, Loss: 8513705.0, Train Accuracy: 0.5162708759307861, Test Accuracy: 0.5799648761749268\n",
      "Epoch 199/300, Loss: 8551631.0, Train Accuracy: 0.49692171812057495, Test Accuracy: 0.5694200396537781\n",
      "Epoch 200/300, Loss: 8638819.0, Train Accuracy: 0.5782761573791504, Test Accuracy: 0.6432337164878845\n",
      "Epoch 201/300, Loss: 8795927.0, Train Accuracy: 0.495162695646286, Test Accuracy: 0.5483304262161255\n",
      "Epoch 202/300, Loss: 8584023.0, Train Accuracy: 0.5, Test Accuracy: 0.5500878691673279\n",
      "Epoch 203/300, Loss: 8696183.0, Train Accuracy: 0.5035180449485779, Test Accuracy: 0.5711774826049805\n",
      "Epoch 204/300, Loss: 8498394.0, Train Accuracy: 0.4727352559566498, Test Accuracy: 0.5307556986808777\n",
      "Epoch 205/300, Loss: 8574277.0, Train Accuracy: 0.4630606770515442, Test Accuracy: 0.5254833102226257\n",
      "Epoch 206/300, Loss: 8667036.0, Train Accuracy: 0.47229552268981934, Test Accuracy: 0.5272407531738281\n",
      "Epoch 207/300, Loss: 9108992.0, Train Accuracy: 0.48504838347435, Test Accuracy: 0.5588752031326294\n",
      "Epoch 208/300, Loss: 8521556.0, Train Accuracy: 0.4964819848537445, Test Accuracy: 0.5694200396537781\n",
      "Epoch 209/300, Loss: 8504672.0, Train Accuracy: 0.4569041430950165, Test Accuracy: 0.5061511397361755\n",
      "Epoch 210/300, Loss: 8329896.0, Train Accuracy: 0.45558488368988037, Test Accuracy: 0.502636194229126\n",
      "Epoch 211/300, Loss: 8423772.0, Train Accuracy: 0.48240986466407776, Test Accuracy: 0.5588752031326294\n",
      "Epoch 212/300, Loss: 8731244.0, Train Accuracy: 0.4766930639743805, Test Accuracy: 0.5342706441879272\n",
      "Epoch 213/300, Loss: 8487166.0, Train Accuracy: 0.5620052814483643, Test Accuracy: 0.6239016056060791\n",
      "Epoch 214/300, Loss: 8607109.0, Train Accuracy: 0.47581353783607483, Test Accuracy: 0.5430579781532288\n",
      "Epoch 215/300, Loss: 9017927.0, Train Accuracy: 0.46657872200012207, Test Accuracy: 0.5237258076667786\n",
      "Epoch 216/300, Loss: 8333691.0, Train Accuracy: 0.4793315827846527, Test Accuracy: 0.5325132012367249\n",
      "Epoch 217/300, Loss: 8444113.0, Train Accuracy: 0.4841688573360443, Test Accuracy: 0.5500878691673279\n",
      "Epoch 218/300, Loss: 8417299.0, Train Accuracy: 0.5378188490867615, Test Accuracy: 0.6186291575431824\n",
      "Epoch 219/300, Loss: 8564383.0, Train Accuracy: 0.46481969952583313, Test Accuracy: 0.5202109217643738\n",
      "Epoch 220/300, Loss: 8548121.0, Train Accuracy: 0.47493404150009155, Test Accuracy: 0.5430579781532288\n",
      "Epoch 221/300, Loss: 8401361.0, Train Accuracy: 0.5452946424484253, Test Accuracy: 0.6080843806266785\n",
      "Epoch 222/300, Loss: 8532031.0, Train Accuracy: 0.48460862040519714, Test Accuracy: 0.5483304262161255\n",
      "Epoch 223/300, Loss: 8462225.0, Train Accuracy: 0.46130168437957764, Test Accuracy: 0.5166959762573242\n",
      "Epoch 224/300, Loss: 8533885.0, Train Accuracy: 0.4678979814052582, Test Accuracy: 0.5254833102226257\n",
      "Epoch 225/300, Loss: 8381477.0, Train Accuracy: 0.47977131605148315, Test Accuracy: 0.551845371723175\n",
      "Epoch 226/300, Loss: 8431648.0, Train Accuracy: 0.49076518416404724, Test Accuracy: 0.562390148639679\n",
      "Epoch 227/300, Loss: 8404514.0, Train Accuracy: 0.47493404150009155, Test Accuracy: 0.5483304262161255\n",
      "Epoch 228/300, Loss: 8500589.0, Train Accuracy: 0.47845205664634705, Test Accuracy: 0.5360281467437744\n",
      "Epoch 229/300, Loss: 8556530.0, Train Accuracy: 0.4656991958618164, Test Accuracy: 0.5114235281944275\n",
      "Epoch 230/300, Loss: 8331241.5, Train Accuracy: 0.5180299282073975, Test Accuracy: 0.5817223191261292\n",
      "Epoch 231/300, Loss: 8727575.0, Train Accuracy: 0.4890061616897583, Test Accuracy: 0.5588752031326294\n",
      "Epoch 232/300, Loss: 8957449.0, Train Accuracy: 0.5145118832588196, Test Accuracy: 0.5641476511955261\n",
      "Epoch 233/300, Loss: 8594255.0, Train Accuracy: 0.4964819848537445, Test Accuracy: 0.5553602576255798\n",
      "Epoch 234/300, Loss: 8733581.0, Train Accuracy: 0.47977131605148315, Test Accuracy: 0.5430579781532288\n",
      "Epoch 235/300, Loss: 8203885.5, Train Accuracy: 0.5540897250175476, Test Accuracy: 0.5922671556472778\n",
      "Epoch 236/300, Loss: 8519924.0, Train Accuracy: 0.48240986466407776, Test Accuracy: 0.5395430326461792\n",
      "Epoch 237/300, Loss: 8397237.0, Train Accuracy: 0.6662269234657288, Test Accuracy: 0.6555360555648804\n",
      "Epoch 238/300, Loss: 8455829.0, Train Accuracy: 0.62269127368927, Test Accuracy: 0.6854130029678345\n",
      "Epoch 239/300, Loss: 8479073.0, Train Accuracy: 0.48812663555145264, Test Accuracy: 0.551845371723175\n",
      "Epoch 240/300, Loss: 8612366.0, Train Accuracy: 0.48504838347435, Test Accuracy: 0.5430579781532288\n",
      "Epoch 241/300, Loss: 8362758.5, Train Accuracy: 0.49560245871543884, Test Accuracy: 0.562390148639679\n",
      "Epoch 242/300, Loss: 8314483.5, Train Accuracy: 0.5149516463279724, Test Accuracy: 0.5852372646331787\n",
      "Epoch 243/300, Loss: 8350195.0, Train Accuracy: 0.4912049174308777, Test Accuracy: 0.5676625370979309\n",
      "Epoch 244/300, Loss: 8574834.0, Train Accuracy: 0.5263852477073669, Test Accuracy: 0.6010544896125793\n",
      "Epoch 245/300, Loss: 8371478.5, Train Accuracy: 0.46481969952583313, Test Accuracy: 0.5096660852432251\n",
      "Epoch 246/300, Loss: 8567146.0, Train Accuracy: 0.48328936100006104, Test Accuracy: 0.5395430326461792\n",
      "Epoch 247/300, Loss: 8713766.0, Train Accuracy: 0.47625330090522766, Test Accuracy: 0.5289982557296753\n",
      "Epoch 248/300, Loss: 9762186.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 249/300, Loss: 8464024.0, Train Accuracy: 0.5153914093971252, Test Accuracy: 0.5782073736190796\n",
      "Epoch 250/300, Loss: 8861069.0, Train Accuracy: 0.4744942784309387, Test Accuracy: 0.5360281467437744\n",
      "Epoch 251/300, Loss: 8536180.0, Train Accuracy: 0.4709762632846832, Test Accuracy: 0.5237258076667786\n",
      "Epoch 252/300, Loss: 8306837.5, Train Accuracy: 0.4705365002155304, Test Accuracy: 0.5289982557296753\n",
      "Epoch 253/300, Loss: 8490093.0, Train Accuracy: 0.4863676428794861, Test Accuracy: 0.551845371723175\n",
      "Epoch 254/300, Loss: 8261208.0, Train Accuracy: 0.4978012442588806, Test Accuracy: 0.5588752031326294\n",
      "Epoch 255/300, Loss: 8354818.0, Train Accuracy: 0.48812663555145264, Test Accuracy: 0.5536028146743774\n",
      "Epoch 256/300, Loss: 8278328.5, Train Accuracy: 0.5228672027587891, Test Accuracy: 0.5905096530914307\n",
      "Epoch 257/300, Loss: 8216511.0, Train Accuracy: 0.5980650782585144, Test Accuracy: 0.6555360555648804\n",
      "Epoch 258/300, Loss: 8304335.5, Train Accuracy: 0.5580474734306335, Test Accuracy: 0.6256590485572815\n",
      "Epoch 259/300, Loss: 8211614.5, Train Accuracy: 0.4643799364566803, Test Accuracy: 0.5360281467437744\n",
      "Epoch 260/300, Loss: 8164952.5, Train Accuracy: 0.47361478209495544, Test Accuracy: 0.5289982557296753\n",
      "Epoch 261/300, Loss: 8291499.0, Train Accuracy: 0.4617414176464081, Test Accuracy: 0.5131810307502747\n",
      "Epoch 262/300, Loss: 8348466.5, Train Accuracy: 0.5378188490867615, Test Accuracy: 0.5992969870567322\n",
      "Epoch 263/300, Loss: 8144160.0, Train Accuracy: 0.5448548793792725, Test Accuracy: 0.5992969870567322\n",
      "Epoch 264/300, Loss: 8372338.5, Train Accuracy: 0.5114336013793945, Test Accuracy: 0.5764499306678772\n",
      "Epoch 265/300, Loss: 8167681.5, Train Accuracy: 0.47141599655151367, Test Accuracy: 0.5219683647155762\n",
      "Epoch 266/300, Loss: 8130701.5, Train Accuracy: 0.48944592475891113, Test Accuracy: 0.5553602576255798\n",
      "Epoch 267/300, Loss: 8410279.0, Train Accuracy: 0.48109057545661926, Test Accuracy: 0.5465729236602783\n",
      "Epoch 268/300, Loss: 8472728.0, Train Accuracy: 0.4986807405948639, Test Accuracy: 0.5641476511955261\n",
      "Epoch 269/300, Loss: 8197406.5, Train Accuracy: 0.5228672027587891, Test Accuracy: 0.5834797620773315\n",
      "Epoch 270/300, Loss: 9759821.0, Train Accuracy: 0.4705365002155304, Test Accuracy: 0.5237258076667786\n",
      "Epoch 271/300, Loss: 8367251.5, Train Accuracy: 0.5923482775688171, Test Accuracy: 0.6274164915084839\n",
      "Epoch 272/300, Loss: 8283175.5, Train Accuracy: 0.5400176048278809, Test Accuracy: 0.6045694351196289\n",
      "Epoch 273/300, Loss: 8327982.0, Train Accuracy: 0.45646438002586365, Test Accuracy: 0.5008787512779236\n",
      "Epoch 274/300, Loss: 8316495.0, Train Accuracy: 0.45646438002586365, Test Accuracy: 0.5079085826873779\n",
      "Epoch 275/300, Loss: 8339537.0, Train Accuracy: 0.4595426619052887, Test Accuracy: 0.5079085826873779\n",
      "Epoch 276/300, Loss: 8478238.0, Train Accuracy: 0.4793315827846527, Test Accuracy: 0.5413005352020264\n",
      "Epoch 277/300, Loss: 8088721.0, Train Accuracy: 0.5206684470176697, Test Accuracy: 0.5852372646331787\n",
      "Epoch 278/300, Loss: 8141021.5, Train Accuracy: 0.46481969952583313, Test Accuracy: 0.5202109217643738\n",
      "Epoch 279/300, Loss: 8092264.0, Train Accuracy: 0.5523306727409363, Test Accuracy: 0.5869947075843811\n",
      "Epoch 280/300, Loss: 9089632.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 281/300, Loss: 8372607.0, Train Accuracy: 0.5378188490867615, Test Accuracy: 0.5957821011543274\n",
      "Epoch 282/300, Loss: 8106631.5, Train Accuracy: 0.4718557596206665, Test Accuracy: 0.5219683647155762\n",
      "Epoch 283/300, Loss: 8153196.0, Train Accuracy: 0.460422158241272, Test Accuracy: 0.5096660852432251\n",
      "Epoch 284/300, Loss: 8186761.0, Train Accuracy: 0.5145118832588196, Test Accuracy: 0.5694200396537781\n",
      "Epoch 285/300, Loss: 9330769.0, Train Accuracy: 0.45866313576698303, Test Accuracy: 0.5114235281944275\n",
      "Epoch 286/300, Loss: 8237452.5, Train Accuracy: 0.46481969952583313, Test Accuracy: 0.5131810307502747\n",
      "Epoch 287/300, Loss: 8663961.0, Train Accuracy: 0.45778363943099976, Test Accuracy: 0.5096660852432251\n",
      "Epoch 288/300, Loss: 8057430.0, Train Accuracy: 0.45998239517211914, Test Accuracy: 0.5131810307502747\n",
      "Epoch 289/300, Loss: 7989380.5, Train Accuracy: 0.5162708759307861, Test Accuracy: 0.5869947075843811\n",
      "Epoch 290/300, Loss: 8049337.0, Train Accuracy: 0.5145118832588196, Test Accuracy: 0.557117760181427\n",
      "Epoch 291/300, Loss: 8089462.0, Train Accuracy: 0.48812663555145264, Test Accuracy: 0.5413005352020264\n",
      "Epoch 292/300, Loss: 8471257.0, Train Accuracy: 0.4617414176464081, Test Accuracy: 0.5184534192085266\n",
      "Epoch 293/300, Loss: 7885943.0, Train Accuracy: 0.5105540752410889, Test Accuracy: 0.5676625370979309\n",
      "Epoch 294/300, Loss: 8901157.0, Train Accuracy: 0.45646438002586365, Test Accuracy: 0.5096660852432251\n",
      "Epoch 295/300, Loss: 8254951.0, Train Accuracy: 0.5303429961204529, Test Accuracy: 0.57469242811203\n",
      "Epoch 296/300, Loss: 8292862.5, Train Accuracy: 0.4876869022846222, Test Accuracy: 0.551845371723175\n",
      "Epoch 297/300, Loss: 7873580.0, Train Accuracy: 0.5074757933616638, Test Accuracy: 0.557117760181427\n",
      "Epoch 298/300, Loss: 9015429.0, Train Accuracy: 0.5061565637588501, Test Accuracy: 0.5606327056884766\n",
      "Epoch 299/300, Loss: 8573314.0, Train Accuracy: 0.5919085144996643, Test Accuracy: 0.6133567690849304\n",
      "Epoch 300/300, Loss: 8105432.0, Train Accuracy: 0.4731750190258026, Test Accuracy: 0.5219683647155762\n",
      "18/18 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Load the input and output data from the pandas DataFrame\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like', 'Pressure', 'External Humidity', 'Dew Point', 'Clouds', 'Wind Speed']]\n",
    "\n",
    "# Convert the pandas DataFrame to numpy arrays with float32 data type\n",
    "X = X.astype('float32').to_numpy()\n",
    "y = y.astype('float32').to_numpy()\n",
    "\n",
    "# Reshape the input data to match the VGG16 input shape\n",
    "X = X.reshape(-1, 6, 1, 1)\n",
    "\n",
    "# Define the VGG-like model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(6, 1, 1)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(3))  # Output layer with 3 units for the 3 output columns\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_y, test_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and compute the loss\n",
    "    history = model.fit(train_X, train_y, batch_size=32, epochs=1, verbose=0)\n",
    "\n",
    "    # Compute training and testing accuracy\n",
    "    train_loss, train_accuracy = model.evaluate(train_X, train_y, verbose=0)\n",
    "    test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=0)\n",
    "\n",
    "    # Print the loss and accuracy after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {history.history['loss'][0]}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Evaluation\n",
    "predictions = model.predict(test_X)\n",
    "# Perform further evaluation or analysis on the predictions\n",
    "\n",
    "# Print the\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LSTM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 15612470.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 2/100, Loss: 14598738.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 3/100, Loss: 12457932.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 4/100, Loss: 11612101.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 5/100, Loss: 11579144.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 6/100, Loss: 11580081.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 7/100, Loss: 11579932.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 8/100, Loss: 11586812.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 9/100, Loss: 11577862.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 10/100, Loss: 11584354.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 11/100, Loss: 11578469.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 12/100, Loss: 11576841.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 13/100, Loss: 11581021.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 14/100, Loss: 11577624.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 15/100, Loss: 11580880.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 16/100, Loss: 11578029.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 17/100, Loss: 11577312.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 18/100, Loss: 11578397.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 19/100, Loss: 11577443.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 20/100, Loss: 11578469.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 21/100, Loss: 11576154.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 22/100, Loss: 11577584.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 23/100, Loss: 11575731.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 24/100, Loss: 11579461.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 25/100, Loss: 11580708.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 26/100, Loss: 11576082.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 27/100, Loss: 11580777.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 28/100, Loss: 11577246.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 29/100, Loss: 11579814.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 30/100, Loss: 11576832.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 31/100, Loss: 11580394.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 32/100, Loss: 11576899.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 33/100, Loss: 11582442.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 34/100, Loss: 11580326.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 35/100, Loss: 11604321.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 36/100, Loss: 11582190.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 37/100, Loss: 11579726.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 38/100, Loss: 11578983.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 39/100, Loss: 11582582.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 40/100, Loss: 11575624.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 41/100, Loss: 11577919.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 42/100, Loss: 11577235.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 43/100, Loss: 11577315.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 44/100, Loss: 11587162.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 45/100, Loss: 11580098.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 46/100, Loss: 11580380.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 47/100, Loss: 11579371.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 48/100, Loss: 11576281.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 49/100, Loss: 11582315.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 50/100, Loss: 11577541.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 51/100, Loss: 11578717.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 52/100, Loss: 11574994.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 53/100, Loss: 11585852.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 54/100, Loss: 11577716.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 55/100, Loss: 11579091.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 56/100, Loss: 11583175.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 57/100, Loss: 11583076.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 58/100, Loss: 11579125.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 59/100, Loss: 11580440.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 60/100, Loss: 11580309.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 61/100, Loss: 11578418.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 62/100, Loss: 11583381.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 63/100, Loss: 11576811.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 64/100, Loss: 11577366.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 65/100, Loss: 11576685.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 66/100, Loss: 11585318.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 67/100, Loss: 11581173.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 68/100, Loss: 11577296.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 69/100, Loss: 11578008.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 70/100, Loss: 11578152.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 71/100, Loss: 11576322.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 72/100, Loss: 11578028.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 73/100, Loss: 11577013.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 74/100, Loss: 11580463.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 75/100, Loss: 11578664.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 76/100, Loss: 11577488.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 77/100, Loss: 11577316.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 78/100, Loss: 11575915.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 79/100, Loss: 11577702.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 80/100, Loss: 11579213.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 81/100, Loss: 11578334.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 82/100, Loss: 11582034.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 83/100, Loss: 11574924.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 84/100, Loss: 11578942.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 85/100, Loss: 11581046.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 86/100, Loss: 11578719.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 87/100, Loss: 11582405.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 88/100, Loss: 11581394.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 89/100, Loss: 11577196.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 90/100, Loss: 11576509.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 91/100, Loss: 11583660.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 92/100, Loss: 11582070.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 93/100, Loss: 11582188.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 94/100, Loss: 11582892.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 95/100, Loss: 11580205.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 96/100, Loss: 11582701.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 97/100, Loss: 11585284.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 98/100, Loss: 11593831.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 99/100, Loss: 11582459.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 100/100, Loss: 11590778.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "18/18 [==============================] - 1s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Load the input and output data from the pandas DataFrame\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like', 'Pressure', 'External Humidity', 'Dew Point', 'Clouds', 'Wind Speed']]\n",
    "\n",
    "# Convert the pandas DataFrame to numpy arrays with float32 data type\n",
    "X = X.astype('float32').to_numpy()\n",
    "y = y.astype('float32').to_numpy()\n",
    "\n",
    "# Define the model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(128, return_sequences=True, input_shape=(None, 6)))\n",
    "model.add(layers.LSTM(128))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(3))  # Output layer with 3 units for the 3 output columns\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Reshape the input data for the RNN\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_y, test_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and compute the loss\n",
    "    history = model.fit(train_X, train_y, batch_size=32, epochs=1, verbose=0)\n",
    "\n",
    "    # Compute training and testing accuracy\n",
    "    train_loss, train_accuracy = model.evaluate(train_X, train_y, verbose=0)\n",
    "    test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=0)\n",
    "\n",
    "    # Print the loss and accuracy after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {history.history['loss'][0]}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Evaluation\n",
    "predictions = model.predict(test_X)\n",
    "# Perform further evaluation or analysis on the predictions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RESNET</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 12964691.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 2/100, Loss: 11614191.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 3/100, Loss: 11596519.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 4/100, Loss: 11618454.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 5/100, Loss: 11592729.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 6/100, Loss: 11594659.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 7/100, Loss: 11594632.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 8/100, Loss: 11562349.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 9/100, Loss: 11606548.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 10/100, Loss: 11599482.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 11/100, Loss: 11554015.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 12/100, Loss: 11547938.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 13/100, Loss: 11548827.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 14/100, Loss: 11555180.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 15/100, Loss: 11534903.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 16/100, Loss: 11519606.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 17/100, Loss: 11525971.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 18/100, Loss: 11494359.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 19/100, Loss: 11507347.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 20/100, Loss: 11522440.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 21/100, Loss: 11459189.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 22/100, Loss: 11431126.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 23/100, Loss: 11443822.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 24/100, Loss: 11409779.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 25/100, Loss: 11329102.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 26/100, Loss: 11276990.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 27/100, Loss: 11347687.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 28/100, Loss: 11248916.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 29/100, Loss: 11216911.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 30/100, Loss: 11084373.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 31/100, Loss: 11049157.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 32/100, Loss: 11025623.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 33/100, Loss: 10936040.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 34/100, Loss: 10895277.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 35/100, Loss: 10765340.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 36/100, Loss: 10682528.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 37/100, Loss: 10684455.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 38/100, Loss: 10576498.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 39/100, Loss: 10586081.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 40/100, Loss: 10570856.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 41/100, Loss: 10570002.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 42/100, Loss: 10659633.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 43/100, Loss: 10701353.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 44/100, Loss: 10675322.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 45/100, Loss: 10714399.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 46/100, Loss: 10569799.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 47/100, Loss: 10518749.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 48/100, Loss: 10571478.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 49/100, Loss: 10568057.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 50/100, Loss: 10558008.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 51/100, Loss: 10516335.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 52/100, Loss: 10561763.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 53/100, Loss: 10597044.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 54/100, Loss: 10513442.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 55/100, Loss: 10539211.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 56/100, Loss: 10520540.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 57/100, Loss: 10477973.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 58/100, Loss: 10497140.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 59/100, Loss: 10505745.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 60/100, Loss: 10488977.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 61/100, Loss: 10532662.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 62/100, Loss: 10583478.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 63/100, Loss: 10499779.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 64/100, Loss: 10452589.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 65/100, Loss: 10411188.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 66/100, Loss: 10395244.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 67/100, Loss: 10508909.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 68/100, Loss: 10404526.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 69/100, Loss: 10383682.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 70/100, Loss: 10369693.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 71/100, Loss: 10377748.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 72/100, Loss: 10309914.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.49384886026382446\n",
      "Epoch 73/100, Loss: 10320621.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4920913875102997\n",
      "Epoch 74/100, Loss: 10374085.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 75/100, Loss: 10345422.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 76/100, Loss: 10236129.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 77/100, Loss: 10181094.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 78/100, Loss: 10198314.0, Train Accuracy: 0.46481969952583313, Test Accuracy: 0.502636194229126\n",
      "Epoch 79/100, Loss: 10162863.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 80/100, Loss: 10118577.0, Train Accuracy: 0.4929639399051666, Test Accuracy: 0.5377855896949768\n",
      "Epoch 81/100, Loss: 10199372.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.49384886026382446\n",
      "Epoch 82/100, Loss: 10183991.0, Train Accuracy: 0.4621811807155609, Test Accuracy: 0.502636194229126\n",
      "Epoch 83/100, Loss: 10123186.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 84/100, Loss: 10099079.0, Train Accuracy: 0.4656991958618164, Test Accuracy: 0.5061511397361755\n",
      "Epoch 85/100, Loss: 10035010.0, Train Accuracy: 0.4595426619052887, Test Accuracy: 0.5008787512779236\n",
      "Epoch 86/100, Loss: 10121313.0, Train Accuracy: 0.4643799364566803, Test Accuracy: 0.5079085826873779\n",
      "Epoch 87/100, Loss: 9980896.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.49384886026382446\n",
      "Epoch 88/100, Loss: 10027532.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 89/100, Loss: 9970647.0, Train Accuracy: 0.5131925940513611, Test Accuracy: 0.5500878691673279\n",
      "Epoch 90/100, Loss: 9920365.0, Train Accuracy: 0.45778363943099976, Test Accuracy: 0.502636194229126\n",
      "Epoch 91/100, Loss: 9911888.0, Train Accuracy: 0.509674608707428, Test Accuracy: 0.5465729236602783\n",
      "Epoch 92/100, Loss: 10006369.0, Train Accuracy: 0.5241864323616028, Test Accuracy: 0.551845371723175\n",
      "Epoch 93/100, Loss: 9906396.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 94/100, Loss: 9917374.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.49384886026382446\n",
      "Epoch 95/100, Loss: 9850232.0, Train Accuracy: 0.47229552268981934, Test Accuracy: 0.5114235281944275\n",
      "Epoch 96/100, Loss: 9805384.0, Train Accuracy: 0.45998239517211914, Test Accuracy: 0.5043936967849731\n",
      "Epoch 97/100, Loss: 9913530.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4920913875102997\n",
      "Epoch 98/100, Loss: 9850660.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.49384886026382446\n",
      "Epoch 99/100, Loss: 9792935.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "Epoch 100/100, Loss: 10031964.0, Train Accuracy: 0.44898855686187744, Test Accuracy: 0.4903339147567749\n",
      "18/18 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Load the input and output data from the pandas DataFrame\n",
    "y = merged_df[['average_internal_temp', 'average_internal_humidity', 'light']]\n",
    "X = merged_df[['Feels Like', 'Pressure', 'External Humidity', 'Dew Point', 'Clouds', 'Wind Speed']]\n",
    "\n",
    "# Convert the pandas DataFrame to numpy arrays with float32 data type\n",
    "X = X.astype('float32').to_numpy()\n",
    "y = y.astype('float32').to_numpy()\n",
    "\n",
    "# Preprocess the input data (optional, depending on your specific use case)\n",
    "# ...\n",
    "\n",
    "# Modify the ResNet architecture for regression\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom regression layers on top of the base model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Input(shape=(6,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(2048, activation='relu'))\n",
    "model.add(layers.Reshape((4, 4, 128)))\n",
    "# model.add(base_model)\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(layers.Dense(3))  # Output layer with 3 units for the 3 output columns\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_y, test_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and compute the loss\n",
    "    history = model.fit(train_X, train_y, batch_size=32, epochs=1, verbose=0)\n",
    "\n",
    "    # Compute training and testing accuracy\n",
    "    train_loss, train_accuracy = model.evaluate(train_X, train_y, verbose=0)\n",
    "    test_loss, test_accuracy = model.evaluate(test_X, test_y, verbose=0)\n",
    "\n",
    "    # Print the loss and accuracy after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {history.history['loss'][0]}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Evaluation\n",
    "predictions = model.predict(test_X)\n",
    "# Perform further evaluation or analysis on the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "ed69033dcf95bf03c49614e5fe4e96f21774bed12a1dae221247a7de4d30fa71"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
